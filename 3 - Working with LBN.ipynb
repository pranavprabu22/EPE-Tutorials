{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Working with LBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial was made in collaboration with Htet Aung Myin and Daniel Sun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBN, or Lorentz Boost Network is a type of machine learning model that uses a four-momenta vector as an input to build the model. \n",
    "\n",
    "LBN can be used to classify both a top tagging jet survey and a five tagger jet survey, but for the purposes of the tutorial, we will be focusing on classifying a top tagging jet survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBN is reliant on the following packages to run:<br>\n",
    "Python3<br>\n",
    "h5py<br>\n",
    "numpy<br>\n",
    "pandas<br>\n",
    "cudatoolkit<br>\n",
    "TensorFlow<br>\n",
    "\n",
    "Make sure to download them using an installer such as pip or conda before working on LBN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering All the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all of the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from lbn import LBN, LBNLayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the number of GPUs available in the computer to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') #Gathers the list of GPUs available to use\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) #Selects the first available device (usually a computer will not have more than one GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up all of the preprocessed data and store it in separate variables before closing the files; there should be one of x_train, y_train, x_test, and y_test before closing all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_X_train = h5py.File('../data/top-tagging/X_train.h5', 'r')\n",
    "X_train = h5f_X_train['data'][:]\n",
    "h5f_X_train.close()\n",
    "\n",
    "h5f_y_train = h5py.File('../data/top-tagging/y_train.h5', 'r')\n",
    "y_train = h5f_y_train['data'][:]\n",
    "h5f_y_train.close()\n",
    "\n",
    "h5f_X_test = h5py.File('../data/top-tagging/X_test.h5', 'r')\n",
    "X_test = h5f_X_test['data'][:]\n",
    "h5f_X_test.close()\n",
    "\n",
    "h5f_y_test = h5py.File('../data/top-tagging/y_test.h5', 'r')\n",
    "y_test = h5f_y_test['data'][:]\n",
    "h5f_y_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the dataset from zenodo and open up the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_hdf('../data/top-tagging/test.h5','table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape #Checking shape of test set to see what values are needed for the input shape of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LBN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes building the actual model. Start with establishing the input shape that you know from the shape of the X_test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (13,4)\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make the actual LBN Layer to begin building the model. If you are curious as to why the layer is made in a specific way, here are the links to the repository for the basic LBN model and the paper to guide you.<br>\n",
    "https://github.com/riga/LBN<br>\n",
    "https://arxiv.org/pdf/1812.09722.pdf<br>\n",
    "Page 3 describes the model shape in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LBNLayer(input_shape, 13, boost_mode=LBN.PAIRS, features=[\"E\", \"pt\", \"eta\", \"phi\", \"m\", \"pair_cos\"])(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add the Dense layers and the output layer to complete the creation of all layers for the model. The activation for the output layer should be sigmoid, as it is resulting in one node. If you make a model that has an output layer made up of more than one node, use softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc1_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc2_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc3_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc4_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc5_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc6_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc7_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='elu', name='fc8_elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "outputs = Dense(1, kernel_initializer='lecun_uniform', kernel_regularizer='l2', activation='sigmoid', name='output_sigmoid')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model itself, along with the Adam optimizer with a learning rate of 0.00001, and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"lbn-zenodo\")\n",
    "adam = Adam(lr=0.00001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, begin training the model. If training each epoch takes too long, change the batch size, and make the number of epochs smaller to ensure you don't leave it running for 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 1024, epochs = 15, \n",
    "                    validation_split = 0.1, shuffle = True, callbacks = None,\n",
    "                    use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the method to plot the learning curve and the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('zenodo-l2_learning_curve.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRoc(features_val, labels_val, labels, model, outputDir='', outputSuffix=''):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    labels_pred = model.predict(features_val)\n",
    "    df = pd.DataFrame()\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "    plt.figure(figsize=(10,8))       \n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = labels_pred[:,i]\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(fpr[label],tpr[label],label='%s tagger, AUC = %.1f%%'%(label.replace('j_',''),auc1[label]*100.))\n",
    "    plt.plot([0, 1], [0, 1], lw=1, color='black', linestyle='--')\n",
    "    #plt.semilogy()\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim(0.001,1.05)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.figtext(0.25, 0.90,'LBN ROC Curve',fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
    "    #plt.figtext(0.35, 0.90,'preliminary', style='italic', wrap=True, horizontalalignment='center', fontsize=14) \n",
    "    #plt.savefig('%sROC_%s.pdf'%(outputDir, outputSuffix))\n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the methods to plot the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = makeRoc(X_test, y_test, ['is_signal_new'], model, outputSuffix='lbn-zenodo-l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have just successfully built a functioning LBN model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
